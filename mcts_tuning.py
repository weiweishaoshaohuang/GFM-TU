#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MCTSè¶…å‚æ•°è°ƒä¼˜å·¥å…·
ç”¨äºè‡ªåŠ¨å¯»æ‰¾æœ€ä½³çš„MCTSå‚æ•°ç»„åˆ
"""

import os
import sys
import json
import time
import itertools
import argparse
from typing import Dict, List, Tuple, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from test_mcts import MCTSTestSuite, create_test_args


class MCTSHyperparameterTuner:
    """MCTSè¶…å‚æ•°è°ƒä¼˜å™¨"""
    
    def __init__(self, base_args):
        """åˆå§‹åŒ–è°ƒä¼˜å™¨"""
        self.base_args = base_args
        self.best_params = None
        self.best_score = 0.0
        self.tuning_results = []
        
        # å®šä¹‰å‚æ•°æœç´¢ç©ºé—´ ç¶²æ ¼
        self.param_space = {
            # 'mcts_c_puct': [0.8, 1.0, 1.4, 2.0, 2.5],           # UCBæ¢ç´¢ç³»æ•°
            # 'mcts_max_simulations': [20, 30, 50, 80, 100],       # æœ€å¤§æ¨¡æ‹Ÿæ¬¡æ•°
            # 'mcts_max_depth': [6, 8, 10, 12],                    # æœ€å¤§æœç´¢æ·±åº¦
            # 'mcts_timeout': [15, 20, 30, 45]                     # æ¨¡æ‹Ÿè¶…æ—¶
            'mcts_c_puct': [0.8, 1.0, 1.2],      
            'mcts_max_simulations': [80, 100, 120],  
            'mcts_max_depth': [8, 10, 12],          
            'mcts_timeout': [15, 20]   
        }
        
        # å®šä¹‰è¯„ä¼°æŒ‡æ ‡æƒé‡
        self.metric_weights = {
            'em_score': 0.4,        # EMåˆ†æ•°æƒé‡
            'llm_score': 0.4,       # LLMåˆ†æ•°æƒé‡
            'success_rate': 0.15,   # æˆåŠŸç‡æƒé‡
            'time_efficiency': 0.05 # æ—¶é—´æ•ˆç‡æƒé‡
        }
    
    def grid_search(self, max_combinations: int = 50) -> Dict:
        """ç½‘æ ¼æœç´¢æœ€ä½³å‚æ•°"""
        print("ğŸ” å¼€å§‹ç½‘æ ¼æœç´¢è¶…å‚æ•°...")
        
        # ç”Ÿæˆå‚æ•°ç»„åˆ
        param_combinations = self._generate_param_combinations(max_combinations)
        print(f"å…±ç”Ÿæˆ{len(param_combinations)}ä¸ªå‚æ•°ç»„åˆ")
        
        # åŠ è½½æµ‹è¯•æ•°æ®ï¼ˆä½¿ç”¨æ›´å°‘çš„æ ·æœ¬è¿›è¡Œè°ƒä¼˜ï¼‰
        test_suite = MCTSTestSuite(self.base_args)
        test_cases = test_suite.load_test_data(num_samples=2)  # è°ƒä¼˜æ—¶ä½¿ç”¨å°‘é‡æ ·æœ¬
        
        if not test_cases:
            print("âŒ æ— æ³•åŠ è½½æµ‹è¯•æ•°æ®")
            return {}
        
        print(f"ä½¿ç”¨{len(test_cases)}ä¸ªæµ‹è¯•æ ·æœ¬è¿›è¡Œè°ƒä¼˜")
        
        # æµ‹è¯•æ¯ä¸ªå‚æ•°ç»„åˆ
        for i, params in enumerate(param_combinations):
            print(f"\n--- æµ‹è¯•å‚æ•°ç»„åˆ {i+1}/{len(param_combinations)} ---")
            print(f"å‚æ•°: {params}")
            
            # æ›´æ–°å‚æ•°
            args = self._update_args_with_params(self.base_args, params)
            
            # è¿è¡Œæµ‹è¯•
            start_time = time.time()
            test_suite_for_params = MCTSTestSuite(args)
            
            try:
                # åªæµ‹è¯•MCTSæ–¹æ³•
                mcts_results = []
                for test_case in test_cases:
                    result = test_suite_for_params.run_single_test(test_case, use_mcts=True)
                    mcts_results.append(result)
                
                end_time = time.time()
                
                # è®¡ç®—ç»¼åˆåˆ†æ•°
                score = self._calculate_combined_score(mcts_results, end_time - start_time)
                
                # è®°å½•ç»“æœ
                tuning_result = {
                    'params': params,
                    'score': score,
                    'results': mcts_results,
                    'total_time': end_time - start_time
                }
                self.tuning_results.append(tuning_result)
                
                print(f"ç»¼åˆåˆ†æ•°: {score:.4f}")
                
                # æ›´æ–°æœ€ä½³å‚æ•°
                if score > self.best_score:
                    self.best_score = score
                    self.best_params = params
                    print(f"ğŸ¯ å‘ç°æ›´ä½³å‚æ•°ç»„åˆï¼åˆ†æ•°: {score:.4f}")
                
            except Exception as e:
                print(f"âŒ å‚æ•°æµ‹è¯•å¤±è´¥: {e}")
                continue
        
        # è¿”å›è°ƒä¼˜ç»“æœ
        tuning_summary = {
            'best_params': self.best_params,
            'best_score': self.best_score,
            'all_results': self.tuning_results,
            'param_analysis': self._analyze_parameter_importance()
        }
        
        return tuning_summary
    
    def _generate_param_combinations(self, max_combinations: int) -> List[Dict]:
        """ç”Ÿæˆå‚æ•°ç»„åˆ"""
        all_combinations = []
        
        # ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„ç»„åˆ
        param_names = list(self.param_space.keys())
        param_values = [self.param_space[name] for name in param_names]
        
        for combination in itertools.product(*param_values):
            params = dict(zip(param_names, combination))
            all_combinations.append(params)
        
        # å¦‚æœç»„åˆæ•°é‡å¤ªå¤šï¼Œéšæœºé€‰æ‹©éƒ¨åˆ†ç»„åˆ
        if len(all_combinations) > max_combinations:
            import random
            random.seed(42)  # ç¡®ä¿å¯é‡ç°
            all_combinations = random.sample(all_combinations, max_combinations)
        
        return all_combinations
    
    def _update_args_with_params(self, base_args, params: Dict):
        """ç”¨æ–°å‚æ•°æ›´æ–°args"""
        import copy
        new_args = copy.deepcopy(base_args)
        
        for param_name, param_value in params.items():
            setattr(new_args, param_name, param_value)
        
        return new_args
    
    def _calculate_combined_score(self, results: List[Dict], total_time: float) -> float:
        """è®¡ç®—ç»¼åˆåˆ†æ•°"""
        successful_results = [r for r in results if r['success']]
        
        if not successful_results:
            return 0.0
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        avg_em = sum(r['em_score'] for r in successful_results) / len(successful_results)
        avg_llm = sum(r['llm_score'] for r in successful_results) / len(successful_results)
        success_rate = len(successful_results) / len(results)
        
        # æ—¶é—´æ•ˆç‡ï¼ˆè¶ŠçŸ­è¶Šå¥½ï¼Œæ ‡å‡†åŒ–åˆ°0-1ï¼‰
        max_reasonable_time = 60  # æœ€å¤§åˆç†æ—¶é—´ï¼ˆç§’ï¼‰
        time_efficiency = max(0, 1.0 - total_time / max_reasonable_time)
        
        # è®¡ç®—åŠ æƒç»¼åˆåˆ†æ•°
        combined_score = (
            self.metric_weights['em_score'] * avg_em +
            self.metric_weights['llm_score'] * avg_llm +
            self.metric_weights['success_rate'] * success_rate +
            self.metric_weights['time_efficiency'] * time_efficiency
        )
        
        return combined_score
    
    def _analyze_parameter_importance(self) -> Dict:
        """åˆ†æå‚æ•°é‡è¦æ€§"""
        if not self.tuning_results:
            return {}
        
        param_analysis = {}
        
        for param_name in self.param_space.keys():
            # æŒ‰å‚æ•°å€¼åˆ†ç»„è®¡ç®—å¹³å‡åˆ†æ•°
            param_scores = {}
            for result in self.tuning_results:
                param_value = result['params'][param_name]
                if param_value not in param_scores:
                    param_scores[param_value] = []
                param_scores[param_value].append(result['score'])
            
            # è®¡ç®—æ¯ä¸ªå‚æ•°å€¼çš„å¹³å‡åˆ†æ•° æ¯”å¦‚'avg_scores_by_value': {0.8: 0.72, 1.0: 0.81, 1.4: 0.83, 2.0: 0.75, 2.5: 0.70} çœ‹å‡º1.4å°æ‡‰çš„å¹³å‡0,83æœ€é«˜
            param_avg_scores = {}
            for value, scores in param_scores.items():
                param_avg_scores[value] = sum(scores) / len(scores)
            
            param_analysis[param_name] = {
                'avg_scores_by_value': param_avg_scores,
                'best_value': max(param_avg_scores.keys(), key=lambda x: param_avg_scores[x]),
                'score_range': max(param_avg_scores.values()) - min(param_avg_scores.values()) # è¶Šå¤§ä»£è¡¨èª¿åƒæ•ˆæœè¶Šå¥½
            }
        
        return param_analysis
    
    def bayesian_optimization(self, n_iterations: int = 20) -> Dict:
        """è´å¶æ–¯ä¼˜åŒ–ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        print("ğŸ§  å¼€å§‹è´å¶æ–¯ä¼˜åŒ–...")
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        test_suite = MCTSTestSuite(self.base_args)
        test_cases = test_suite.load_test_data(num_samples=2)
        
        if not test_cases:
            print("âŒ æ— æ³•åŠ è½½æµ‹è¯•æ•°æ®")
            return {}
        
        # åˆå§‹éšæœºé‡‡æ ·
        initial_samples = 5
        random_params = self._generate_param_combinations(initial_samples)
        
        for i, params in enumerate(random_params):
            print(f"\n--- åˆå§‹é‡‡æ · {i+1}/{initial_samples} ---")
            score = self._evaluate_params(params, test_cases)
            
            tuning_result = {
                'params': params,
                'score': score,
                'iteration': i,
                'method': 'random_init'
            }
            self.tuning_results.append(tuning_result)
            
            if score > self.best_score:
                self.best_score = score
                self.best_params = params
        
        # è´å¶æ–¯ä¼˜åŒ–è¿­ä»£ï¼ˆç®€åŒ–ä¸ºå±€éƒ¨æœç´¢ï¼‰
        for iteration in range(n_iterations - initial_samples):
            print(f"\n--- è´å¶æ–¯ä¼˜åŒ–è¿­ä»£ {iteration+1}/{n_iterations-initial_samples} ---")
            
            # åŸºäºå½“å‰æœ€ä½³å‚æ•°è¿›è¡Œå±€éƒ¨æœç´¢
            candidate_params = self._generate_local_candidates(self.best_params)
            
            best_candidate_score = 0
            best_candidate_params = None
            
            for params in candidate_params:
                score = self._evaluate_params(params, test_cases)
                
                tuning_result = {
                    'params': params,
                    'score': score,
                    'iteration': initial_samples + iteration,
                    'method': 'bayesian'
                }
                self.tuning_results.append(tuning_result)
                
                if score > best_candidate_score:
                    best_candidate_score = score
                    best_candidate_params = params
            
            # æ›´æ–°æœ€ä½³å‚æ•°
            if best_candidate_score > self.best_score:
                self.best_score = best_candidate_score
                self.best_params = best_candidate_params
                print(f"ğŸ¯ è´å¶æ–¯ä¼˜åŒ–å‘ç°æ›´ä½³å‚æ•°ï¼åˆ†æ•°: {best_candidate_score:.4f}")
        
        return {
            'best_params': self.best_params,
            'best_score': self.best_score,
            'optimization_history': self.tuning_results
        }
    
    def _evaluate_params(self, params: Dict, test_cases: List[Dict]) -> float:
        """è¯„ä¼°å‚æ•°ç»„åˆ"""
        args = self._update_args_with_params(self.base_args, params)
        test_suite = MCTSTestSuite(args)
        
        try:
            start_time = time.time()
            results = []
            
            for test_case in test_cases:
                result = test_suite.run_single_test(test_case, use_mcts=True)
                results.append(result)
            
            end_time = time.time()
            score = self._calculate_combined_score(results, end_time - start_time)
            
            print(f"å‚æ•° {params} -> åˆ†æ•°: {score:.4f}")
            return score
            
        except Exception as e:
            print(f"å‚æ•°è¯„ä¼°å¤±è´¥: {e}")
            return 0.0
    
    def _generate_local_candidates(self, base_params: Dict, n_candidates: int = 5) -> List[Dict]:
        """ç”Ÿæˆå±€éƒ¨å€™é€‰å‚æ•° ç”ŸæˆN_candidateå€‹æ•¸æ“šå…¶ä¸­æ•¸æ“šç…§base_paramséš¨æ©Ÿæ‰¾ä¸€å€‹åƒæ•¸å°‡ä»–å¾€å³æˆ–å¾€å·¦èª¿"""
        candidates = []
        
        for _ in range(n_candidates):
            candidate = base_params.copy()
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªå‚æ•°è¿›è¡Œè°ƒæ•´
            import random
            param_to_adjust = random.choice(list(self.param_space.keys()))
            current_value = candidate[param_to_adjust]
            possible_values = self.param_space[param_to_adjust]
            
            # é€‰æ‹©ç›¸é‚»çš„å€¼
            current_index = possible_values.index(current_value)
            
            # å‘å·¦æˆ–å‘å³ç§»åŠ¨
            if random.random() < 0.5 and current_index > 0:
                candidate[param_to_adjust] = possible_values[current_index - 1]
            elif current_index < len(possible_values) - 1:
                candidate[param_to_adjust] = possible_values[current_index + 1]
            
            candidates.append(candidate)
        
        return candidates
    
    def print_tuning_report(self, tuning_results: Dict):
        """æ‰“å°è°ƒä¼˜æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ¯ MCTSè¶…å‚æ•°è°ƒä¼˜æŠ¥å‘Š")
        print("="*80)
        
        if not tuning_results.get('best_params'):
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å‚æ•°ç»„åˆ")
            return
        
        print(f"\nğŸ† æœ€ä½³å‚æ•°ç»„åˆ:")
        best_params = tuning_results['best_params']
        best_score = tuning_results['best_score']
        
        for param, value in best_params.items():
            print(f"  {param}: {value}")
        print(f"  ç»¼åˆåˆ†æ•°: {best_score:.4f}")
        
        # å‚æ•°é‡è¦æ€§åˆ†æ
        if 'param_analysis' in tuning_results:
            print(f"\nğŸ“Š å‚æ•°é‡è¦æ€§åˆ†æ:")
            param_analysis = tuning_results['param_analysis']
            
            for param_name, analysis in param_analysis.items():
                best_value = analysis['best_value']
                score_range = analysis['score_range']
                print(f"  {param_name}:")
                print(f"    æœ€ä½³å€¼: {best_value}")
                print(f"    å½±å“ç¨‹åº¦: {score_range:.4f}")
        
        # æ¨èé…ç½®
        print(f"\nğŸ’¡ æ¨èé…ç½®:")
        print("å»ºè®®åœ¨main.pyæˆ–é…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨ä»¥ä¸‹å‚æ•°:")
        for param, value in best_params.items():
            print(f"  --{param} {value}")


def create_tuning_args():
    """åˆ›å»ºè°ƒä¼˜å‚æ•°"""
    # åŸºäºæµ‹è¯•å‚æ•°åˆ›å»º
    args = create_test_args()
    
    # ä¿®æ”¹ä¸ºä½¿ç”¨AIT-QAæ•°æ®é›†
    args.dataset = "ait-qa"
    args.qa_path = "dataset/AIT-QA/aitqa_clean_questions.json"
    args.table_folder = ""  # AIT-QAä¸éœ€è¦table_folder
    args.embed_cache_dir = "dataset/AIT-QA/"
    
    # è°ƒä¼˜ç‰¹å®šå‚æ•°
    args.test_samples = 2  # è°ƒä¼˜æ—¶ä½¿ç”¨è¾ƒå°‘æ ·æœ¬
    args.debug = False     # è°ƒä¼˜æ—¶å…³é—­è°ƒè¯•è¾“å‡º
    
    return args


def main():
    """ä¸»è°ƒä¼˜å‡½æ•°"""
    print("ğŸ›ï¸ å¯åŠ¨MCTSè¶…å‚æ•°è°ƒä¼˜...")
    
    # åˆ›å»ºåŸºç¡€å‚æ•°
    base_args = create_tuning_args()
    
    # æ£€æŸ¥API key
    if not base_args.key:
        print("âŒ é”™è¯¯: éœ€è¦æœ‰æ•ˆçš„API key")
        print("è§£å†³æ–¹æ¡ˆ:")
        print("1. ä½¿ç”¨start.batä¸­çš„é…ç½®ï¼ˆåº”è¯¥å·²è‡ªåŠ¨åŠ è½½ï¼‰")
        print("2. æˆ–è€…ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°: --key your-api-key")
        print(f"3. å½“å‰ä½¿ç”¨æ¨¡å‹: {base_args.model}")
        print(f"4. å½“å‰base_url: {base_args.base_url}")
        return
    
    # æ£€æŸ¥æ•°æ®é›†
    if not os.path.exists(base_args.qa_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ•°æ®é›†: {base_args.qa_path}")
        return
    
    # åˆ›å»ºè°ƒä¼˜å™¨
    tuner = MCTSHyperparameterTuner(base_args)
    
    # é€‰æ‹©è°ƒä¼˜æ–¹æ³•
    tuning_method = input("\né€‰æ‹©è°ƒä¼˜æ–¹æ³• (1: ç½‘æ ¼æœç´¢, 2: è´å¶æ–¯ä¼˜åŒ–): ").strip()
    
    if tuning_method == "1":
        print("ğŸ” ä½¿ç”¨ç½‘æ ¼æœç´¢...")
        max_combinations = int(input("æœ€å¤§å‚æ•°ç»„åˆæ•° (æ¨è20-50): ").strip() or "30")
        tuning_results = tuner.grid_search(max_combinations)
    elif tuning_method == "2":
        print("ğŸ§  ä½¿ç”¨è´å¶æ–¯ä¼˜åŒ–...")
        n_iterations = int(input("ä¼˜åŒ–è¿­ä»£æ¬¡æ•° (æ¨è15-30): ").strip() or "20")
        tuning_results = tuner.bayesian_optimization(n_iterations)
    else:
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤ç½‘æ ¼æœç´¢")
        tuning_results = tuner.grid_search(30)
    
    # æ‰“å°è°ƒä¼˜æŠ¥å‘Š
    tuner.print_tuning_report(tuning_results)
    
    # ä¿å­˜ç»“æœ
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    result_file = f"mcts_tuning_results_{timestamp}.json"
    
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(tuning_results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ è°ƒä¼˜ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    print("\nâœ… è¶…å‚æ•°è°ƒä¼˜å®Œæˆï¼")


if __name__ == '__main__':
    main() 