#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MCTSç³»ç»Ÿæµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯MCTSå®ç°çš„æ­£ç¡®æ€§å’Œæ€§èƒ½å¯¹æ¯”
"""

import os
import sys
import argparse
import json
import time
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from configs import GEMINI_KEYS, hyperparameter
from GraphRetriever.graph_retriver import GraphRetriever
from Generator.openai_api import ChatGPTTool
from GraphRetriever.dense_retriever import load_dense_retriever
from Generator.Gemini_model import GeminiTool
from iterative_reasoning import GraphReasoner
from mcts_adapter import MCTSGraphReasoner
from compute_score import eval_ex_match, LLM_eval

def str2bool(v):
    if isinstance(v, bool):
        return v
    v = v.strip().lower()
    if v in ("1","true","t","yes","y"):
        return True
    if v in ("0","false","f","no","n"):
        return False
    raise argparse.ArgumentTypeError("Boolean value expected (true/false, 1/0)")

class MCTSTestSuite:
    """MCTSæµ‹è¯•å¥—ä»¶"""
    
    def __init__(self, args):
        self.args = args
        self.results = {
            'traditional': [],
            'mcts': [],
            'comparison': {}
        }
        
        # åŠ è½½æ¨¡å‹
        self.model, self.dense_retriever = self._load_model()
        
    def _load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if 'gemini' in self.args.model:
            gemini_key_index = 0
            gemini_key = GEMINI_KEYS[gemini_key_index]
            model = GeminiTool(gemini_key, self.args)
        else:
            model = ChatGPTTool(self.args)
        
        dense_retriever = load_dense_retriever(self.args)
        return model, dense_retriever
    
    def load_test_data(self, num_samples: int = 5) -> List[Dict]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        test_cases = []
        
        if self.args.dataset in ('hitab', 'Hitab'):
            import jsonlines
            with open(self.args.qa_path, "r+", encoding='utf-8') as f:
                qas = [item for item in jsonlines.Reader(f)]
            
            # å–å‰å‡ ä¸ªæ ·æœ¬è¿›è¡Œæµ‹è¯•
            selected_qas = qas[:num_samples]
            
            for qa in selected_qas:
                table_path = self.args.table_folder + qa['table_id'] + '.json'
                with open(table_path, "r+", encoding='utf-8') as f:
                    table = json.load(f)
                
                test_case = {
                    'query': qa['question'],
                    'answer': '|'.join([str(i) for i in qa['answer']]),
                    'table_caption': table['title'],
                    'table': table['texts'],
                    'table_path': table_path
                }
                test_cases.append(test_case)
        
        elif self.args.dataset in ('AIT-QA', 'ait-qa'):
            with open(self.args.qa_path, 'r', encoding='utf-8') as f:
                qas = json.load(f)
            
            selected_qas = qas[:num_samples]
            
            for qa in selected_qas:
                test_case = {
                    'query': qa['question'],
                    'answer': '|'.join([str(i) for i in qa['answers']]),
                    'table_caption': '',
                    'table': qa['table'],
                    'table_path': qa
                }
                test_cases.append(test_case)
        
        print(f"åŠ è½½äº†{len(test_cases)}ä¸ªæµ‹è¯•ç”¨ä¾‹")
        return test_cases
    
    def run_single_test(self, test_case: Dict, use_mcts: bool = True) -> Dict:
        """è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨ä¾‹"""
        query = test_case['query']
        answer = test_case['answer']
        caption = test_case['table_caption']
        table = test_case['table']
        table_path = test_case['table_path']
        
        print(f"\n{'='*60}")
        print(f"é—®é¢˜: {query}")
        print(f"æ­£ç¡®ç­”æ¡ˆ: {answer}")
        print(f"æ¨ç†æ–¹æ³•: {'MCTS' if use_mcts else 'ä¼ ç»Ÿ'}")
        print(f"{'='*60}")
        
        start_time = time.time()
        
        try:
            # åˆ›å»ºå›¾æ£€ç´¢å™¨
            graph_retriever = GraphRetriever(table_path, self.model, self.dense_retriever, 
                                           self.args.dataset, table_cation=caption)
            
            # åˆ›å»ºæ¨ç†å™¨
            if use_mcts:
                graph_reasoner = MCTSGraphReasoner(self.args, self.model, query, table, 
                                                 caption, graph_retriever, self.args.dataset)
            else:
                graph_reasoner = GraphReasoner(self.args, self.model, query, table, 
                                             caption, graph_retriever, self.args.dataset)
            
            # æ‰§è¡Œæ¨ç†
            output = graph_reasoner.iterative_reasoning()
            
            end_time = time.time()
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            em_score = eval_ex_match(output, answer)
            llm_score = LLM_eval(self.model, query, output, answer)
            
            result = {
                'query': query,
                'predicted_answer': output,
                'ground_truth': answer,
                'em_score': em_score,
                'llm_score': llm_score,
                'time_cost': end_time - start_time,
                'method': 'MCTS' if use_mcts else 'Traditional',
                'success': True
            }
            
            # å¦‚æœæ˜¯MCTSï¼Œæ·»åŠ ç»Ÿè®¡ä¿¡æ¯
            if use_mcts and hasattr(graph_reasoner, 'get_stats'):
                result['mcts_stats'] = graph_reasoner.get_stats()
            
            print(f"æ¨¡å‹å›ç­”: {output}")
            print(f"EMåˆ†æ•°: {em_score}")
            print(f"LLMåˆ†æ•°: {llm_score}")
            print(f"è€—æ—¶: {end_time - start_time:.2f}ç§’")
            
            return result
            
        except Exception as e:
            print(f"æµ‹è¯•å¤±è´¥: {e}")
            return {
                'query': query,
                'error': str(e),
                'success': False,
                'method': 'MCTS' if use_mcts else 'Traditional',
                'time_cost': time.time() - start_time
            }
    
    def run_comparison_test(self, test_cases: List[Dict]) -> Dict:
        """è¿è¡Œå¯¹æ¯”æµ‹è¯•"""
        print("\nğŸ”„ å¼€å§‹å¯¹æ¯”æµ‹è¯•...")
        
        traditional_results = []
        mcts_results = []
        
        for i, test_case in enumerate(test_cases):
            print(f"\n--- æµ‹è¯•ç”¨ä¾‹ {i+1}/{len(test_cases)} ---")
            
            # ä¼ ç»Ÿæ–¹æ³•æµ‹è¯•
            print("ğŸ”¹ æµ‹è¯•ä¼ ç»Ÿæ–¹æ³•...")
            traditional_result = self.run_single_test(test_case, use_mcts=False)
            traditional_results.append(traditional_result)
            
            # MCTSæ–¹æ³•æµ‹è¯•
            print("\nğŸ”¹ æµ‹è¯•MCTSæ–¹æ³•...")
            mcts_result = self.run_single_test(test_case, use_mcts=True)
            mcts_results.append(mcts_result)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        comparison = self._calculate_comparison_stats(traditional_results, mcts_results)
        
        return {
            'traditional_results': traditional_results,
            'mcts_results': mcts_results,
            'comparison': comparison
        }
    
    def _calculate_comparison_stats(self, traditional_results: List[Dict], mcts_results: List[Dict]) -> Dict:
        """è®¡ç®—å¯¹æ¯”ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'traditional': self._calculate_method_stats(traditional_results),
            'mcts': self._calculate_method_stats(mcts_results),
            'improvement': {}
        }
        
        # è®¡ç®—æ”¹è¿›ç¨‹åº¦
        trad_stats = stats['traditional']
        mcts_stats = stats['mcts']
        
        if trad_stats['avg_em'] > 0:
            stats['improvement']['em_improvement'] = (mcts_stats['avg_em'] - trad_stats['avg_em']) / trad_stats['avg_em'] * 100
        else:
            stats['improvement']['em_improvement'] = 0
        
        if trad_stats['avg_llm'] > 0:
            stats['improvement']['llm_improvement'] = (mcts_stats['avg_llm'] - trad_stats['avg_llm']) / trad_stats['avg_llm'] * 100
        else:
            stats['improvement']['llm_improvement'] = 0
        
        stats['improvement']['time_change'] = mcts_stats['avg_time'] - trad_stats['avg_time']
        stats['improvement']['success_rate_change'] = mcts_stats['success_rate'] - trad_stats['success_rate']
        
        return stats
    
    def _calculate_method_stats(self, results: List[Dict]) -> Dict:
        """è®¡ç®—å•ä¸ªæ–¹æ³•çš„ç»Ÿè®¡ä¿¡æ¯"""
        successful_results = [r for r in results if r['success']]
        
        if not successful_results:
            return {
                'total_cases': len(results),
                'successful_cases': 0,
                'success_rate': 0.0,
                'avg_em': 0.0,
                'avg_llm': 0.0,
                'avg_time': 0.0
            }
        
        total_em = sum(r['em_score'] for r in successful_results)
        total_llm = sum(r['llm_score'] for r in successful_results)
        total_time = sum(r['time_cost'] for r in successful_results)
        
        return {
            'total_cases': len(results),
            'successful_cases': len(successful_results),
            'success_rate': len(successful_results) / len(results),
            'avg_em': total_em / len(successful_results),
            'avg_llm': total_llm / len(successful_results),
            'avg_time': total_time / len(successful_results)
        }
    
    def print_final_report(self, comparison_results: Dict):
        """æ‰“å°æœ€ç»ˆæŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ¯ MCTS vs ä¼ ç»Ÿæ–¹æ³•å¯¹æ¯”æŠ¥å‘Š")
        print("="*80)
        
        trad_stats = comparison_results['comparison']['traditional']
        mcts_stats = comparison_results['comparison']['mcts']
        improvement = comparison_results['comparison']['improvement']
        
        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"{'æŒ‡æ ‡':<20} {'ä¼ ç»Ÿæ–¹æ³•':<15} {'MCTSæ–¹æ³•':<15} {'æ”¹è¿›(%)':<10}")
        print("-" * 70)
        print(f"{'æˆåŠŸç‡':<20} {trad_stats['success_rate']:<15.3f} {mcts_stats['success_rate']:<15.3f} {improvement['success_rate_change']*100:<10.2f}")
        print(f"{'å¹³å‡EMåˆ†æ•°':<20} {trad_stats['avg_em']:<15.3f} {mcts_stats['avg_em']:<15.3f} {improvement['em_improvement']:<10.2f}")
        print(f"{'å¹³å‡LLMåˆ†æ•°':<20} {trad_stats['avg_llm']:<15.3f} {mcts_stats['avg_llm']:<15.3f} {improvement['llm_improvement']:<10.2f}")
        print(f"{'å¹³å‡è€—æ—¶(ç§’)':<20} {trad_stats['avg_time']:<15.2f} {mcts_stats['avg_time']:<15.2f} {improvement['time_change']:<10.2f}")
        
        print(f"\nğŸ† ç»“è®º:")
        if improvement['em_improvement'] > 0:
            print(f"âœ… MCTSåœ¨EMåˆ†æ•°ä¸Šæå‡äº† {improvement['em_improvement']:.2f}%")
        else:
            print(f"âŒ MCTSåœ¨EMåˆ†æ•°ä¸Šä¸‹é™äº† {abs(improvement['em_improvement']):.2f}%")
        
        if improvement['llm_improvement'] > 0:
            print(f"âœ… MCTSåœ¨LLMåˆ†æ•°ä¸Šæå‡äº† {improvement['llm_improvement']:.2f}%")
        else:
            print(f"âŒ MCTSåœ¨LLMåˆ†æ•°ä¸Šä¸‹é™äº† {abs(improvement['llm_improvement']):.2f}%")
        
        if improvement['time_change'] < 0:
            print(f"âš¡ MCTSåœ¨æ—¶é—´ä¸ŠèŠ‚çœäº† {abs(improvement['time_change']):.2f}ç§’")
        else:
            print(f"â° MCTSåœ¨æ—¶é—´ä¸Šå¢åŠ äº† {improvement['time_change']:.2f}ç§’")
        
        # MCTSè¯¦ç»†ç»Ÿè®¡
        mcts_results = comparison_results['mcts_results']
        mcts_detailed = [r for r in mcts_results if r['success'] and 'mcts_stats' in r]
        
        if mcts_detailed:
            print(f"\nğŸŒ² MCTSè¯¦ç»†ç»Ÿè®¡:")
            avg_simulations = sum(r['mcts_stats']['iterations'] for r in mcts_detailed) / len(mcts_detailed)
            print(f"å¹³å‡æ¨¡æ‹Ÿæ¬¡æ•°: {avg_simulations:.1f}")
            
            methods_used = [r['mcts_stats']['method_used'] for r in mcts_detailed]
            mcts_usage = sum(1 for m in methods_used if 'MCTS' in m) / len(methods_used) * 100
            print(f"MCTSä½¿ç”¨ç‡: {mcts_usage:.1f}%")


def create_test_args():
    """åˆ›å»ºæµ‹è¯•å‚æ•°"""
    parser = argparse.ArgumentParser()
    
    # åŸºæœ¬å‚æ•° - å¤ç”¨start.batä¸­çš„é…ç½®
    parser.add_argument("--model", type=str, default="qwen2-7b-instruct")
    parser.add_argument("--base_url", type=str, default="https://dashscope.aliyuncs.com/compatible-mode/v1")
    parser.add_argument("--key", type=str, default="", help="API key (å¤ç”¨start.baté…ç½®)")
    parser.add_argument("--dataset", type=str, default="ait-qa")
    parser.add_argument("--qa_path", type=str, default="dataset/AIT-QA/aitqa_clean_questions.json")
    parser.add_argument("--table_folder", type=str, default="")
    
    parser.add_argument("--max_iteration_depth", type=int, default=5)
    parser.add_argument("--temperature", type=float, default=0.0)
    parser.add_argument("--seed", type=int, default=42)
    
    parser.add_argument("--embed_cache_dir", type=str, default="dataset/AIT-QA/")
    parser.add_argument("--embedder_path", type=str, default="thenlper/gte-base")  # å¤ç”¨start.baté…ç½®
    
    parser.add_argument("--debug", type=str2bool, default=True)
    
    # MCTSå‚æ•°
    parser.add_argument("--use_mcts", type=str2bool, default=True)
    parser.add_argument("--mcts_c_puct", type=float, default=0.8)  # è°ƒä¼˜æœ€ä¼˜å€¼
    parser.add_argument("--mcts_max_simulations", type=int, default=80)  # è°ƒä¼˜æœ€ä¼˜å€¼
    parser.add_argument("--mcts_max_depth", type=int, default=12)  # è°ƒä¼˜æœ€ä¼˜å€¼
    parser.add_argument("--mcts_timeout", type=int, default=20)  # è°ƒä¼˜æœ€ä¼˜å€¼
    
    # æµ‹è¯•å‚æ•°
    parser.add_argument("--test_samples", type=int, default=3, help="æµ‹è¯•æ ·æœ¬æ•°é‡")
    parser.add_argument("--save_results", type=str2bool, default=True, help="æ˜¯å¦ä¿å­˜ç»“æœ")
    
    return parser.parse_args()


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨MCTSç³»ç»Ÿæµ‹è¯•...")
    
    # åˆ›å»ºæµ‹è¯•å‚æ•°
    args = create_test_args()
    
    # æ£€æŸ¥API key
    if not args.key:
        print("âŒ é”™è¯¯: éœ€è¦æœ‰æ•ˆçš„API key")
        print("è§£å†³æ–¹æ¡ˆ:")
        print("1. ä½¿ç”¨start.batä¸­çš„é…ç½®ï¼ˆå·²è‡ªåŠ¨åŠ è½½ï¼‰")
        print("2. æˆ–è€…ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°: --key your-api-key")
        print(f"3. å½“å‰ä½¿ç”¨æ¨¡å‹: {args.model}")
        print(f"4. å½“å‰base_url: {args.base_url}")
        return
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    if not os.path.exists(args.qa_path):
        print(f"âŒ æ‰¾ä¸åˆ°QAæ–‡ä»¶: {args.qa_path}")
        print("è¯·ç¡®ä¿å·²ä¸‹è½½å¹¶é…ç½®äº†æ•°æ®é›†æ–‡ä»¶")
        return
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = MCTSTestSuite(args)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_cases = test_suite.load_test_data(args.test_samples)
    
    if not test_cases:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•ç”¨ä¾‹")
        return
    
    # è¿è¡Œå¯¹æ¯”æµ‹è¯•
    comparison_results = test_suite.run_comparison_test(test_cases)
    
    # æ‰“å°æœ€ç»ˆæŠ¥å‘Š
    test_suite.print_final_report(comparison_results)
    
    # ä¿å­˜ç»“æœ
    if args.save_results:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        result_file = f"mcts_test_results_{timestamp}.json"
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {result_file}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆ!")


if __name__ == '__main__':
    main()